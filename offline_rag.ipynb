{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Offline RAG System with Ollama\n",
    "\n",
    "##  Overview\n",
    "\n",
    "This notebook implements a **complete Retrieval-Augmented Generation (RAG) system** that runs entirely offline.\n",
    "\n",
    "### What is RAG?\n",
    "- **Retrieval**: Finding relevant information from your documents\n",
    "- **Augmented**: Using that information to enhance the AI's knowledge\n",
    "- **Generation**: Creating accurate answers based on your documents\n",
    "\n",
    "### Prerequisites\n",
    "1. Install Ollama: https://ollama.com/download\n",
    "2. Run: `ollama pull llama3.2`\n",
    "3. Run: `ollama pull nomic-embed-text`\n",
    "4. Create a `documents` folder and add your PDF/Markdown/HTML files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "### Library Choices Explained:\n",
    "\n",
    "| Library | Why This One? | Alternatives |\n",
    "|---------|---------------|-------------|\n",
    "| **faiss-cpu** | Fastest vector search, battle-tested, works offline | ChromaDB (heavier), LanceDB (newer) |\n",
    "| **numpy** | Industry standard for arrays, required by FAISS | No real alternative |\n",
    "| **PyPDF2** | Pure Python, no dependencies, simple | pdfplumber (slower), PyMuPDF (C dependencies) |\n",
    "| **beautifulsoup4** | Best HTML parser, robust | lxml (harder install), html.parser (basic) |\n",
    "| **markdown** | Clean MD to text conversion | mistune (overkill), regex (error-prone) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\hp\\offline-rag\\offline-rag-system\\.venv\\lib\\site-packages (25.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\offline-rag\\offline-rag-system\\.venv\\lib\\site-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\hp\\offline-rag\\offline-rag-system\\.venv\\lib\\site-packages (0.45.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\hp\\offline-rag\\offline-rag-system\\.venv\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\offline-rag\\offline-rag-system\\.venv\\lib\\site-packages (2.3.4)\n",
      "Requirement already satisfied: PyPDF2==3.0.1 in c:\\users\\hp\\offline-rag\\offline-rag-system\\.venv\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: beautifulsoup4==4.12.2 in c:\\users\\hp\\offline-rag\\offline-rag-system\\.venv\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: markdown==3.4.4 in c:\\users\\hp\\offline-rag\\offline-rag-system\\.venv\\lib\\site-packages (3.4.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\offline-rag\\offline-rag-system\\.venv\\lib\\site-packages (from beautifulsoup4==4.12.2) (2.8)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\offline-rag\\offline-rag-system\\.venv\\lib\\site-packages (from faiss-cpu) (25.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages \n",
    "!pip install --upgrade pip setuptools wheel\n",
    "!pip install faiss-cpu numpy PyPDF2==3.0.1 beautifulsoup4==4.12.2 markdown==3.4.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all libraries sucessfully imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "#for document processing\n",
    "import PyPDF2\n",
    "from bs4 import BeautifulSoup\n",
    "import markdown\n",
    "\n",
    "#for vector operations\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "print(\"all libraries sucessfully imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Structures\n",
    "\n",
    "### Why Dataclasses?\n",
    "- Type safety catches errors early\n",
    "- Self-documenting\n",
    "- Less boilerplate than classes\n",
    "- Better than dicts for structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Text chunk with metadata and embedding.\"\"\"\n",
    "    id: str\n",
    "    text: str\n",
    "    vector: Optional[np.ndarray]\n",
    "    metadata: Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 4: Document Loading\n",
    "\n",
    "### Design Decisions:\n",
    "1. **Static methods**: No instance state needed\n",
    "2. **Separate per format**: Easier to extend\n",
    "3. **Error handling**: Continues on failure\n",
    "4. **Page-level tracking**: Enables precise citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document loader ready!\n"
     ]
    }
   ],
   "source": [
    "class DocumentLoader:\n",
    "    \"\"\"load PDF, Markdown, and HTML documents.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_pdf(file_path: str) -> List[Dict]:\n",
    "        \"\"\"extract text from PDF,  page by page for citations.\"\"\"\n",
    "        chunks = []\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file, strict=False)\n",
    "                print(f\"Processing {os.path.basename(file_path)}: {len(pdf_reader.pages)} pages\")\n",
    "\n",
    "                for page_num, page in enumerate(pdf_reader.pages):\n",
    "                    try:\n",
    "                        text = page.extract_text()\n",
    "                        if text and text.strip():\n",
    "                            chunks.append({\n",
    "                                'text': text,\n",
    "                                'metadata': {\n",
    "                                    'source': os.path.basename(file_path),\n",
    "                                    'page': page_num + 1,\n",
    "                                    'type': 'pdf'\n",
    "                                }\n",
    "                            })\n",
    "                            print(f\"  Page {page_num + 1}: extracted {len(text)} characters\")\n",
    "                        else:\n",
    "                            print(f\"  Page {page_num + 1}: no text found\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Page {page_num + 1}: error - {e}\")\n",
    "                        continue\n",
    "\n",
    "                print(f\"Successfully extracted {len(chunks)} pages from {os.path.basename(file_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"error loading PDF {file_path}: {e}\")\n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_markdown(file_path: str) -> List[Dict]:\n",
    "        \"\"\"convert Markdown to text via HTML.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                md_content = file.read()\n",
    "                html = markdown.markdown(md_content)\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                text = soup.get_text()\n",
    "                \n",
    "                return [{\n",
    "                    'text': text,\n",
    "                    'metadata': {\n",
    "                        'source': os.path.basename(file_path),\n",
    "                        'page': 1,\n",
    "                        'type': 'markdown'\n",
    "                    }\n",
    "                }]\n",
    "        except Exception as e:\n",
    "            print(f\"error loading Markdown {file_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_html(file_path: str) -> List[Dict]:\n",
    "        \"\"\"extract text from HTML, removing scripts and styles.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                soup = BeautifulSoup(file.read(), 'html.parser')\n",
    "                for script in soup([\"script\", \"style\"]):\n",
    "                    script.decompose()\n",
    "                text = soup.get_text()\n",
    "                \n",
    "                return [{\n",
    "                    'text': text,\n",
    "                    'metadata': {\n",
    "                        'source': os.path.basename(file_path),\n",
    "                        'page': 1,\n",
    "                        'type': 'html'\n",
    "                    }\n",
    "                }]\n",
    "        except Exception as e:\n",
    "            print(f\"error loading HTML {file_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_documents(directory: str) -> List[Dict]:\n",
    "        \"\"\" load all supported documents from a directory.\"\"\"\n",
    "        documents = []\n",
    "        doc_dir = Path(directory)\n",
    "        \n",
    "        if not doc_dir.exists():\n",
    "            print(f\"Creating {directory}...\")\n",
    "            doc_dir.mkdir(parents=True)\n",
    "            print(f\"add documents to {directory} and run again.\")\n",
    "            return documents\n",
    "        \n",
    "        for file_path in doc_dir.rglob('*'):\n",
    "            if file_path.is_file():\n",
    "                ext = file_path.suffix.lower()\n",
    "                \n",
    "                if ext == '.pdf':\n",
    "                    documents.extend(DocumentLoader.load_pdf(str(file_path)))\n",
    "                elif ext in ['.md', '.markdown']:\n",
    "                    documents.extend(DocumentLoader.load_markdown(str(file_path)))\n",
    "                elif ext in ['.html', '.htm']:\n",
    "                    documents.extend(DocumentLoader.load_html(str(file_path)))\n",
    "        \n",
    "        print(f\"loaded {len(documents)} document sections\")\n",
    "        return documents\n",
    "\n",
    "print(\"document loader ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Text Chunking\n",
    "\n",
    "### Why Chunking?\n",
    "- **Problem**: Documents are too long for embeddings (token limits)\n",
    "- **Solution**: Split into smaller, overlapping pieces\n",
    "\n",
    "### Why Overlap?\n",
    "- Prevents information loss at boundaries\n",
    "- Example: \"...safety protocol. First, wear PPE...\" \n",
    "  - Without overlap: \"First, wear PPE\" loses context\n",
    "  - With overlap: Previous chunk includes \"safety protocol\"\n",
    "\n",
    "### Chunk Size Choice (750 chars):\n",
    "- **Too small** (200): Loses context\n",
    "- **Too large** (2000): Less precise retrieval\n",
    "- **750**: Sweet spot for most documents (~150 tokens)\n",
    "\n",
    "### Sentence Boundary Detection:\n",
    "- Breaks at periods, not mid-sentence\n",
    "- Better comprehension by LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text chunker ready!\n"
     ]
    }
   ],
   "source": [
    "class TextChunker:\n",
    "    \"\"\"text chunking with overlap and sentence boundaries.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"normalize whitespace and remove special characters.\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text)  #multiple spaces to single space\n",
    "        text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\-\\:\\;]', '', text)  #keep punctuation\n",
    "        return text.strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def chunk_text(\n",
    "        text: str,\n",
    "        chunk_size: int = 750,\n",
    "        overlap: int = 100,\n",
    "        metadata: Dict = None\n",
    "    ) -> List[Chunk]:\n",
    "        \"\"\"split text into overlapping chunks at sentence boundaries.\n",
    "        \n",
    "        Args:\n",
    "            chunk_size: target size (â‰ˆ150 tokens for embeddings)\n",
    "            overlap: overlap size to preserve context\n",
    "            metadata: source info for citations\n",
    "        \"\"\"\n",
    "        text = TextChunker.clean_text(text)\n",
    "        chunks = []\n",
    "        \n",
    "        if not text:\n",
    "            return chunks\n",
    "        \n",
    "        start = 0\n",
    "        chunk_index = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            \n",
    "            #break at sentence boundary(last 20% of chunk)\n",
    "            if end < len(text):\n",
    "                search_start = end - int(chunk_size * 0.2)\n",
    "                sentence_end = max(\n",
    "                    text.rfind('.', search_start, end),\n",
    "                    text.rfind('!', search_start, end),\n",
    "                    text.rfind('?', search_start, end)\n",
    "                )\n",
    "                \n",
    "                if sentence_end != -1 and sentence_end > start:\n",
    "                    end = sentence_end + 1\n",
    "            \n",
    "            chunk_text = text[start:end].strip()\n",
    "            \n",
    "            if chunk_text:\n",
    "                chunk_metadata = metadata.copy() if metadata else {}\n",
    "                chunk_metadata['chunk_index'] = chunk_index\n",
    "                chunk_id = f\"{chunk_metadata.get('source', 'unknown')}_{chunk_index}\"\n",
    "                \n",
    "                chunks.append(Chunk(\n",
    "                    id=chunk_id,\n",
    "                    text=chunk_text,\n",
    "                    vector=None,\n",
    "                    metadata=chunk_metadata\n",
    "                ))\n",
    "                \n",
    "                chunk_index += 1\n",
    "            \n",
    "            start = end - overlap  #move with overlap\n",
    "            \n",
    "            if start >= len(text) - overlap:\n",
    "                break\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "print(\"text chunker ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Ollama Embedder\n",
    "\n",
    "### What are Embeddings?\n",
    "- Convert text â†’ vector of numbers (768 dimensions)\n",
    "- Similar text â†’ similar vectors\n",
    "- Enables semantic search\n",
    "\n",
    "### Why Ollama?\n",
    "- **Runs locally**: No API keys, no cloud\n",
    "- **Free**: No usage costs\n",
    "- **Private**: Data never leaves your machine\n",
    "\n",
    "### Why nomic-embed-text?\n",
    "- **Size**: 274MB (lightweight)\n",
    "- **Quality**: Good accuracy for general text\n",
    "- **Dimensions**: 768 (standard size)\n",
    "- **Alternative**: all-MiniLM-L6-v2 (384 dims, smaller but less accurate)\n",
    "\n",
    "### HTTP API vs CLI:\n",
    "- **HTTP**: Direct, faster, better for production\n",
    "- **CLI**: Subprocess overhead\n",
    "- **Choice**: HTTP API for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedder ready!\n"
     ]
    }
   ],
   "source": [
    "class OllamaEmbedder:\n",
    "    \"\"\"Generate embeddings using Ollama's embedding model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"nomic-embed-text\"):\n",
    "        self.model_name = model_name\n",
    "        self._verify_model()\n",
    "    \n",
    "    def _verify_model(self):\n",
    "        \"\"\"Check if model is available locally (no automatic download).\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['ollama', 'list'],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                check=True\n",
    "            )\n",
    "            if self.model_name not in result.stdout:\n",
    "                raise RuntimeError(\n",
    "                    f\"Model '{self.model_name}' not found locally.\\n\"\n",
    "                    f\"Please download it first using:\\n\"\n",
    "                    f\"  ollama pull {self.model_name}\\n\"\n",
    "                    f\"This is a one-time setup step that requires internet connection.\"\n",
    "                )\n",
    "            print(f\"Found embedding model: {self.model_name}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Cannot connect to Ollama service.\\n\"\n",
    "                f\"Please ensure Ollama is installed and running.\\n\"\n",
    "                f\"Error: {e}\"\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            raise RuntimeError(\n",
    "                \"Ollama not found on your system.\\n\"\n",
    "                \"Please install Ollama from: https://ollama.com/download\\n\"\n",
    "                \"This is a one-time setup step.\"\n",
    "            )\n",
    "    \n",
    "    def embed_text(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Generate embedding vector for text using HTTP API.\"\"\"\n",
    "        try:\n",
    "            import http.client\n",
    "            \n",
    "            conn = http.client.HTTPConnection(\"localhost\", 11434, timeout=30)\n",
    "            headers = {'Content-Type': 'application/json'}\n",
    "            \n",
    "            payload = json.dumps({\n",
    "                \"model\": self.model_name,\n",
    "                \"prompt\": text\n",
    "            })\n",
    "            \n",
    "            conn.request(\"POST\", \"/api/embeddings\", payload, headers)\n",
    "            response = conn.getresponse()\n",
    "            data = json.loads(response.read().decode())\n",
    "            \n",
    "            return np.array(data['embedding'], dtype=np.float32)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Embedding error: {e}\")\n",
    "            return np.zeros(768, dtype=np.float32)  # Fallback\n",
    "    \n",
    "    def embed_chunks(self, chunks: List[Chunk]) -> List[Chunk]:\n",
    "        \"\"\"generate embeddings for all chunks with progress.\"\"\"\n",
    "        print(f\"Generating embeddings for {len(chunks)} chunks...\")\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if i % 10 == 0 and i > 0:\n",
    "                print(f\" progress: {i}/{len(chunks)}\")\n",
    "            chunk.vector = self.embed_text(chunk.text)\n",
    "        \n",
    "        print(\"embeddings complete!\")\n",
    "        return chunks\n",
    "\n",
    "print(\"embedder ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Vector Database\n",
    "\n",
    "### Why FAISS?\n",
    "- **Speed**: Fastest similarity search in Python\n",
    "- **Offline**: No external services\n",
    "- **Simple**: IndexFlatL2 = exact search, no tuning\n",
    "- **Mature**: Used by Meta/Facebook in production\n",
    "\n",
    "### FAISS Index Types:\n",
    "- **IndexFlatL2**: Exact search, small datasets (<1M vectors)\n",
    "- **IndexIVFFlat**: Approximate, faster for large datasets\n",
    "- **IndexHNSW**: Graph-based, very fast\n",
    "- **Choice**: Flat for simplicity and accuracy\n",
    "\n",
    "### L2 Distance:\n",
    "- Euclidean distance between vectors\n",
    "- Lower = more similar\n",
    "- Alternative: Cosine similarity (similar results, more complex)\n",
    "\n",
    "### Persistence:\n",
    "- Save index + metadata separately\n",
    "- Vectors in binary (fast)\n",
    "- Metadata in JSON (readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector database ready!\n"
     ]
    }
   ],
   "source": [
    "class VectorDatabase:\n",
    "    \"\"\"FAISS-based vector storage and retrieval with Cosine Similarity.\"\"\"\n",
    "    \n",
    "    def __init__(self, dimension: int = 768):\n",
    "        self.dimension = dimension\n",
    "        #ue IndexFlatIP for cosine similarity\n",
    "        self.index = faiss.IndexFlatIP(dimension) \n",
    "        self.chunks: List[Chunk] = []\n",
    "    \n",
    "    def add_chunks(self, chunks: List[Chunk]):\n",
    "        \"\"\"add chunk embeddings to the index.\"\"\"\n",
    "        vectors = np.array([chunk.vector for chunk in chunks], dtype=np.float32)\n",
    "        \n",
    "        #normalize vectors for cosine similarity\n",
    "        faiss.normalize_L2(vectors)\n",
    "        \n",
    "        self.index.add(vectors)\n",
    "        self.chunks.extend(chunks)\n",
    "        print(f\"Added {len(chunks)} chunks (total: {len(self.chunks)})\")\n",
    "    \n",
    "    def search(self, query_vector: np.ndarray, top_k: int = 5) -> List[Tuple[Chunk, float]]:\n",
    "        \"\"\"find top-k most similar chunks using cosine similarity.\n",
    "        \n",
    "        Returns:\n",
    "            List of (chunk, distance) tuples\n",
    "            Distance is (1 - cosine_similarity), so lower = more similar\n",
    "        \"\"\"\n",
    "        query_vector = query_vector.reshape(1, -1).astype(np.float32)\n",
    "        \n",
    "        #normalize query vector for cosine similarity\n",
    "        faiss.normalize_L2(query_vector)\n",
    "        \n",
    "        #search (returns similarity scores, not distances)\n",
    "        similarities, indices = self.index.search(query_vector, top_k)\n",
    "        \n",
    "        results = []\n",
    "        for idx, similarity in zip(indices[0], similarities[0]):\n",
    "            if idx < len(self.chunks):\n",
    "                #convert similarity to distance: distance = 1 - similarity\n",
    "                distance = 1 - similarity\n",
    "                results.append((self.chunks[idx], float(distance)))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save(self, directory: str):\n",
    "        \"\"\"persist database to disk.\"\"\"\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        #save FAISS index\n",
    "        faiss.write_index(self.index, os.path.join(directory, 'faiss.index'))\n",
    "        \n",
    "        #save chunks metadata (JSON)\n",
    "        chunks_data = [{'id': chunk.id,\n",
    "            'text': chunk.text,\n",
    "            'metadata': chunk.metadata\n",
    "        } for chunk in self.chunks]\n",
    "        \n",
    "        with open(os.path.join(directory, 'chunks.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunks_data, f, indent=2)\n",
    "        \n",
    "        print(f\"database saved to {directory}\")\n",
    "    \n",
    "    def load(self, directory: str, embedder) -> bool:\n",
    "        \"\"\"load database from disk.\"\"\"\n",
    "        index_path = os.path.join(directory, 'faiss.index')\n",
    "        chunks_path = os.path.join(directory, 'chunks.json')\n",
    "        \n",
    "        if not os.path.exists(index_path) or not os.path.exists(chunks_path):\n",
    "            print(f\"no database found in {directory}\")\n",
    "            return False\n",
    "        \n",
    "        #load FAISS index\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        \n",
    "        #load chunks\n",
    "        with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "            chunks_data = json.load(f)\n",
    "        \n",
    "        #reconstruct chunks (re-embed for consistency)\n",
    "        print(\"reconstructing chunk vectors...\")\n",
    "        self.chunks = []\n",
    "        for data in chunks_data:\n",
    "            chunk = Chunk(\n",
    "                id=data['id'],\n",
    "                text=data['text'],\n",
    "                vector=embedder.embed_text(data['text']),\n",
    "                metadata=data['metadata']\n",
    "            )\n",
    "            self.chunks.append(chunk)\n",
    "        \n",
    "        print(f\"database loaded: {len(self.chunks)} chunks\")\n",
    "        return True\n",
    "\n",
    "print(\"vector database ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 8: LLM Interface\n",
    "\n",
    "### Why Llama 3?\n",
    "- **Quality**: State-of-the-art open-source model\n",
    "- **Size**: 4.7GB (manageable on consumer hardware)\n",
    "- **Offline**: Runs locally\n",
    "- **Fast**: Decent speed on CPU\n",
    "\n",
    "### Temperature Parameter:\n",
    "- **0.0**: Deterministic (always same answer)\n",
    "- **0.3**: Slightly creative (good for QA)\n",
    "- **0.7**: More creative (good for writing)\n",
    "- **1.0**: Very creative (can hallucinate)\n",
    "- **Choice**: 0.3 balances accuracy and naturalness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM interface ready!\n"
     ]
    }
   ],
   "source": [
    "class OllamaLLM:\n",
    "    \"\"\"LLM interface using Ollama CLI (more reliable on CPU).\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"llama3.2\"):\n",
    "        self.model_name = model_name\n",
    "        self._verify_model()\n",
    "    \n",
    "    def _verify_model(self):\n",
    "        \"\"\"Check if model is available locally.\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['ollama', 'list'],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                check=True\n",
    "            )\n",
    "            if self.model_name not in result.stdout:\n",
    "                raise RuntimeError(\n",
    "                    f\"Model '{self.model_name}' not found locally.\\n\"\n",
    "                    f\"Please download it first using:\\n\"\n",
    "                    f\"  ollama pull {self.model_name}\\n\"\n",
    "                    f\"This is a one-time setup step that requires internet connection.\"\n",
    "                )\n",
    "            print(f\"Found LLM model: {self.model_name}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Cannot connect to Ollama service.\\n\"\n",
    "                f\"Please ensure Ollama is installed and running.\\n\"\n",
    "                f\"Error: {e}\"\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            raise RuntimeError(\n",
    "                \"Ollama not found on your system.\\n\"\n",
    "                \"Please install Ollama from: https://ollama.com/download\\n\"\n",
    "                \"This is a one-time setup step.\"\n",
    "            )\n",
    "    \n",
    "    def generate(self, prompt: str, temperature: float = 0.3) -> str:\n",
    "        \"\"\"generate response using Ollama CLI (more reliable on CPU).        \n",
    "        Args:\n",
    "            prompt: Complete prompt with context and question\n",
    "            temperature: Creativity (0.0=deterministic, 1.0=creative)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"  Generating with {self.model_name} ...\")\n",
    "            \n",
    "            # Use subprocess with CLI - more reliable than HTTP on CPU\n",
    "            result = subprocess.run(\n",
    "                ['ollama', 'run', self.model_name],\n",
    "                input=prompt,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=300,  # 5 minutes timeout\n",
    "                encoding='utf-8'\n",
    "            )\n",
    "            \n",
    "            if result.returncode != 0:\n",
    "                error_msg = result.stderr or \"Unknown error\"\n",
    "                print(f\" Ollama error: {error_msg}\")\n",
    "                return f\"Error: {error_msg}\"\n",
    "            \n",
    "            answer = result.stdout.strip()\n",
    "            \n",
    "            if not answer:\n",
    "                print(f\" Empty response\")\n",
    "                return \"Error: Empty response from LLM\"\n",
    "            \n",
    "            print(f\" Generated {len(answer)} characters\")\n",
    "            return answer\n",
    "            \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(f\"  Timeout after 5 minutes\")\n",
    "            return \"Error: Generation timed out. Try a simpler question or smaller context.\"\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error: {str(e)}\"\n",
    "            print(f\"  {error_msg}\")\n",
    "            return error_msg\n",
    "\n",
    "print(\"LLM interface ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Complete RAG System\n",
    "\n",
    "### System Architecture:\n",
    "1. **Ingest**: Load docs â†’ Chunk â†’ Embed â†’ Store\n",
    "2. **Query**: Question â†’ Embed â†’ Search â†’ Retrieve chunks\n",
    "3. **Generate**: Build prompt with context â†’ LLM â†’ Answer\n",
    "\n",
    "### Distance Threshold:\n",
    "- Filters out irrelevant chunks\n",
    "- **1.5**: Moderate strictness\n",
    "- Lower = stricter (may miss relevant info)\n",
    "- Higher = lenient (may include noise)\n",
    "\n",
    "### Prompt Engineering:\n",
    "- Clear instructions: \"Answer only from context\"\n",
    "- Structured format: CONTEXT â†’ QUESTION â†’ INSTRUCTIONS\n",
    "- Citation requirement: Forces source attribution\n",
    "- Safety: Refuses to guess without context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG System class ready!\n"
     ]
    }
   ],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"complete RAG orchestration.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        documents_dir: str = \"documents\",\n",
    "        db_dir: str = \"vector_db\",\n",
    "        llm_model: str = \"llama3.2\",\n",
    "        embedding_model: str = \"nomic-embed-text\"\n",
    "    ):\n",
    "        self.documents_dir = documents_dir\n",
    "        self.db_dir = db_dir\n",
    "        \n",
    "        print(\"initializing RAG System...\")\n",
    "        self.embedder = OllamaEmbedder(embedding_model)\n",
    "        self.llm = OllamaLLM(llm_model)\n",
    "        self.vector_db = VectorDatabase()\n",
    "        print(\"RAG System initialized!\")\n",
    "    \n",
    "    def ingest_documents(\n",
    "        self,\n",
    "        chunk_size: int = 750,\n",
    "        overlap: int = 100,\n",
    "        force_rebuild: bool = False\n",
    "    ):\n",
    "        \"\"\"Build or load vector database.\"\"\"\n",
    "        \n",
    "        #try loading existing database\n",
    "        if not force_rebuild and os.path.exists(self.db_dir):\n",
    "            print(\"loading existing database...\")\n",
    "            if self.vector_db.load(self.db_dir, self.embedder):\n",
    "                return\n",
    "        \n",
    "        print(\"Building new database...\")\n",
    "        \n",
    "        #load documents\n",
    "        documents = DocumentLoader.load_documents(self.documents_dir)\n",
    "        if not documents:\n",
    "            print(\"no documents found!\")\n",
    "            return\n",
    "        \n",
    "        #chunk documents\n",
    "        all_chunks = []\n",
    "        for doc in documents:\n",
    "            chunks = TextChunker.chunk_text(\n",
    "                doc['text'],\n",
    "                chunk_size=chunk_size,\n",
    "                overlap=overlap,\n",
    "                metadata=doc['metadata']\n",
    "            )\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        print(f\"created {len(all_chunks)} chunks\")\n",
    "        \n",
    "        #generate embeddings\n",
    "        all_chunks = self.embedder.embed_chunks(all_chunks)\n",
    "        \n",
    "        #store in vector DB\n",
    "        self.vector_db.add_chunks(all_chunks)\n",
    "        \n",
    "        #save for future use\n",
    "        self.vector_db.save(self.db_dir)\n",
    "    \n",
    "    def query(\n",
    "        self,\n",
    "        question: str,\n",
    "        top_k: int = 5,\n",
    "        distance_threshold: float = 1.5\n",
    "    ) -> Dict:\n",
    "        \"\"\"Answer question using RAG.\n",
    "        \n",
    "        Returns:\n",
    "            {\n",
    "                'answer': Generated answer,\n",
    "                'sources': List of source chunks,\n",
    "                'confidence': 'high'|'medium'|'low'\n",
    "            }\n",
    "        \"\"\"\n",
    "        print(f\"\\n Question: {question}\")\n",
    "        \n",
    "        #embed query\n",
    "        query_vector = self.embedder.embed_text(question)\n",
    "        \n",
    "        #search vector DB\n",
    "        results = self.vector_db.search(query_vector, top_k=top_k)\n",
    "        \n",
    "        #filter by threshold\n",
    "        filtered_results = [\n",
    "            (chunk, dist) for chunk, dist in results\n",
    "            if dist < distance_threshold\n",
    "        ]\n",
    "        \n",
    "        if not filtered_results:\n",
    "            return {\n",
    "                'answer': \"Insufficient context to answer this question.\",\n",
    "                'sources': [],\n",
    "                'confidence': 'low'\n",
    "            }\n",
    "        \n",
    "        #build context from chunks\n",
    "        context_parts = []\n",
    "        sources = []\n",
    "        \n",
    "        for i, (chunk, distance) in enumerate(filtered_results):\n",
    "            context_parts.append(\n",
    "                f\"[Source {i+1}: {chunk.metadata['source']}, \"\n",
    "                f\"Page {chunk.metadata.get('page', 'N/A')}]\\n{chunk.text}\\n\"\n",
    "            )\n",
    "            sources.append({\n",
    "                'id': chunk.id,\n",
    "                'source': chunk.metadata['source'],\n",
    "                'page': chunk.metadata.get('page', 'N/A'),\n",
    "                'distance': distance\n",
    "            })\n",
    "        \n",
    "        context = \"\\n\".join(context_parts)\n",
    "        \n",
    "        #build prompt\n",
    "        prompt = f\"\"\"You are a helpful AI assistant. Answer the question based ONLY on the provided context.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Answer based only on the context above\n",
    "2. Cite source numbers (e.g., \"According to Source 1...\")\n",
    "3. If context is insufficient, state that clearly\n",
    "4. Be concise but thorough\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "        \n",
    "        #generate answer\n",
    "        print(\"Generating answer...\")\n",
    "        answer = self.llm.generate(prompt, temperature=0.3)\n",
    "        \n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'confidence': 'high' if len(filtered_results) >= 3 else 'medium'\n",
    "        }\n",
    "\n",
    "print(\"RAG System class ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ® Step 10: Initialize System\n",
    "\n",
    "### Configuration Options:\n",
    "- `documents_dir`: Where your documents are\n",
    "- `db_dir`: Where vector database is saved\n",
    "- `chunk_size`: 500-1000 (750 is balanced)\n",
    "- `overlap`: 10-20% of chunk_size\n",
    "- `force_rebuild`: Set True to rebuild from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing RAG System...\n",
      "Found embedding model: nomic-embed-text\n",
      "Found LLM model: llama3.2\n",
      "RAG System initialized!\n",
      "Building new database...\n",
      "Processing flora.pdf: 10 pages\n",
      "  Page 1: extracted 4059 characters\n",
      "  Page 2: extracted 4664 characters\n",
      "  Page 3: extracted 2931 characters\n",
      "  Page 4: extracted 3098 characters\n",
      "  Page 5: extracted 3477 characters\n",
      "  Page 6: extracted 3609 characters\n",
      "  Page 7: extracted 3364 characters\n",
      "  Page 8: extracted 3834 characters\n",
      "  Page 9: extracted 4666 characters\n",
      "  Page 10: extracted 1916 characters\n",
      "Successfully extracted 10 pages from flora.pdf\n",
      "loaded 10 document sections\n",
      "created 61 chunks\n",
      "Generating embeddings for 61 chunks...\n",
      "  Progress: 10/61\n",
      "  Progress: 20/61\n",
      "  Progress: 30/61\n",
      "  Progress: 40/61\n",
      "  Progress: 50/61\n",
      "  Progress: 60/61\n",
      "embeddings complete!\n",
      "Added 61 chunks (total: 61)\n",
      "database saved to vector_db\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAG system\n",
    "rag = RAGSystem(\n",
    "    documents_dir=\"documents\",\n",
    "    db_dir=\"vector_db\",\n",
    "    llm_model=\"llama3.2\",\n",
    "    embedding_model=\"nomic-embed-text\"\n",
    ")\n",
    "\n",
    "# Build/load database\n",
    "rag.ingest_documents(\n",
    "    chunk_size=750,\n",
    "    overlap=100,\n",
    "    force_rebuild=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¬ Step 11: Ask Questions!\n",
    "\n",
    "### Tips for Good Questions:\n",
    "- Be specific\n",
    "- Use keywords from your documents\n",
    "- One topic per question\n",
    "\n",
    "### Tuning Parameters:\n",
    "- `top_k`: More = more context, slower\n",
    "- `distance_threshold`: Lower = stricter matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Question: What problem does FLoRA aim to address in the context of parameter-efficient fine-tuning (PEFT) for large language models?\n",
      "Generating answer...\n",
      "  Generating with llama3.2 ...\n",
      " Generated 411 characters\n",
      "\n",
      "============================================================\n",
      "ANSWER:\n",
      "============================================================\n",
      "According to Sources 3 and 4, FLoRA aims to address the problem of parameter-efficient fine-tuning for large language models (LLMs) by proposing a family of fused forward-backward adapters (FFBAs) that combine ideas from LoRA and parallel adapters to improve fine-tuning accuracies. Specifically, it seeks to reduce inference-time latencies while maintaining or improving accuracy for similar parameter budgets.\n",
      "\n",
      "============================================================\n",
      "CONFIDENCE: HIGH\n",
      "============================================================\n",
      "\n",
      "SOURCES:\n",
      "  1. flora.pdf (Page 9) - Distance: 0.2233\n",
      "  2. flora.pdf (Page 9) - Distance: 0.2614\n",
      "  3. flora.pdf (Page 1) - Distance: 0.2660\n",
      "  4. flora.pdf (Page 1) - Distance: 0.2805\n",
      "  5. flora.pdf (Page 10) - Distance: 0.2863\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#example: Ask a question\n",
    "question = \"What problem does FLoRA aim to address in the context of parameter-efficient fine-tuning (PEFT) for large language models?\"\n",
    "\n",
    "result = rag.query(\n",
    "    question=question,\n",
    "    top_k=5,\n",
    "    distance_threshold=0.6\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\"*60)\n",
    "print(result['answer'])\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"CONFIDENCE: {result['confidence'].upper()}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nSOURCES:\")\n",
    "for i, source in enumerate(result['sources'], 1):\n",
    "    print(f\"  {i}. {source['source']} (Page {source['page']}) - Distance: {source['distance']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Batch Processing (Optional)\n",
    "\n",
    "Process multiple questions at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Question 1/3\n",
      "============================================================\n",
      "\n",
      " Question: What is the main topic?\n",
      "Generating answer...\n",
      "  Generating with llama3.2 ...\n",
      " Generated 1048 characters\n",
      "Q: What is the main topic?\n",
      "A: Based on the provided context, it appears that the main topic revolves around advancements and studies related to natural language processing (NLP) and large language models, particularly in the areas...\n",
      "Confidence: high\n",
      "Sources: 3\n",
      "\n",
      "============================================================\n",
      "Question 2/3\n",
      "============================================================\n",
      "\n",
      " Question: What are the key points discussed?\n",
      "Generating answer...\n",
      "  Generating with llama3.2 ...\n",
      " Generated 883 characters\n",
      "Q: What are the key points discussed?\n",
      "A: Based on the provided context, it appears that there are multiple papers related to parameter-efficient fine-tuning and prompt tuning for natural language processing tasks.\n",
      "\n",
      "The key points discussed i...\n",
      "Confidence: high\n",
      "Sources: 3\n",
      "\n",
      "============================================================\n",
      "Question 3/3\n",
      "============================================================\n",
      "\n",
      " Question: Are there any specific recommendations?\n",
      "Generating answer...\n",
      "  Generating with llama3.2 ...\n",
      " Generated 808 characters\n",
      "Q: Are there any specific recommendations?\n",
      "A: Based on the provided context, there are no specific recommendations mentioned in the text for recommendations or best practices. The text appears to be a summary of existing research and approaches r...\n",
      "Confidence: high\n",
      "Sources: 3\n"
     ]
    }
   ],
   "source": [
    "# List of questions\n",
    "questions = [\n",
    "    \"What is the main topic?\",\n",
    "    \"What are the key points discussed?\",\n",
    "    \"Are there any specific recommendations?\"\n",
    "]\n",
    "\n",
    "# Process all questions\n",
    "for i, q in enumerate(questions, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question {i}/{len(questions)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = rag.query(q, top_k=3)\n",
    "    \n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {result['answer'][:200]}...\")\n",
    "    print(f\"Confidence: {result['confidence']}\")\n",
    "    print(f\"Sources: {len(result['sources'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 13: Advanced Customization\n",
    "\n",
    "### Adjust Chunk Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For code or technical docs: smaller chunks\n",
    "# rag.ingest_documents(chunk_size=500, overlap=75, force_rebuild=True)\n",
    "\n",
    "# For long-form content: larger chunks\n",
    "# rag.ingest_documents(chunk_size=1000, overlap=150, force_rebuild=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More context (slower, more comprehensive)\n",
    "# result = rag.query(question, top_k=10, distance_threshold=2.0)\n",
    "\n",
    "# Less context (faster, more focused)\n",
    "# result = rag.query(question, top_k=3, distance_threshold=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster but smaller model\n",
    "# rag_fast = RAGSystem(llm_model=\"llama3.2\")\n",
    "\n",
    "# Alternative model\n",
    "# rag_alt = RAGSystem(llm_model=\"mistral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Database Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get database stats\n",
    "print(f\"Total chunks: {len(rag.vector_db.chunks)}\")\n",
    "\n",
    "# Count by source\n",
    "sources = {}\n",
    "for chunk in rag.vector_db.chunks:\n",
    "    source = chunk.metadata['source']\n",
    "    sources[source] = sources.get(source, 0) + 1\n",
    "\n",
    "print(\"\\nChunks per document:\")\n",
    "for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {source}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Save/Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "def save_qa_results(questions_and_answers, filename=\"qa_results.json\"):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(questions_and_answers, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"âœ… Results saved to {filename}\")\n",
    "\n",
    "# Example:\n",
    "# qa_pairs = []\n",
    "# for q in questions:\n",
    "#     result = rag.query(q)\n",
    "#     qa_pairs.append({'question': q, 'answer': result['answer']})\n",
    "# save_qa_results(qa_pairs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
